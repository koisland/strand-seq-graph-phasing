import os
from pathlib import Path

sample = config["sample"]
print(sample)
ss_dir = config["strandseq_dir"]
ss_suffix = config["strandseq_suffix"]
# Rscript = config["Rscript"]
whatshap_split = config["whatshap_split"] # not verified
BubbleGun = config["whatshap_split"] # config["BubbleGun"]
#hifiasm = config["hifiasm"]
# long_reads = config["long_reads_dir"]
#
# cheating = config["cheating"]
# numClustersHard = config["numClustersHard"]
# numClustersSoft = config["numClustersSoft"]
EMiter          = config["EMiter"]
segment_length_threshold          = config["segmentLengthThreshold"]
# alpha           = config["alpha"]
# minLib          = config["minLib"]
# upperQ          = config["upperQ"]
# logLth          = config["logLth"]
# theta_constrain = config["theta_constrain"]
# log_scale    = config["log_scale"]
# numAlignments   = config["numAlignments"]
# sex = config['sex']

# chromosomes = range(1, config["num_chromosomes"]+1)

num_components=config["num_components"]
components = range(1, num_components+1)
# it is expected that the acrocentrics are assigned component 1, as they are the largest component
non_acrocentric_components = range(2, num_components+1)
non_acrocentric_components2 = range(1, num_components-1)
# if sex == 'male' chromosomes += 1
# haplotypes = ["h1","h2"]

assembly = config['assembly']
# assembly_stem=Path(assembly).stem # Without extension
# sample_assembly = sample+'_'+assembly

reference = config['reference']
reference_stem = Path(reference).stem # Without extension
reference_name = os.path.basename(reference) # With extension
reference_ext = Path(reference).suffix # Extension


bwa_index_suffices = ["amb", "ann", "bwt", "pac", "sa"]

outputfolder    = "SaaRclust_results"
libs, = glob_wildcards(os.path.join(ss_dir, "{lib}_1"+ss_suffix))


# Libs Subsets
num_libs = config["numLibs"]
libs = libs[0:num_libs]
print(libs)
# print('dir=', ss_dir)
# print('files =', os.listdir(ss_dir))
# print("libs =", libs)
# print("ss_suffix =", ss_suffix)
# print(assembly_stem)
# print('libs', libs)
# print('ss_dir', ss_dir)
#####################################################################

wildcard_constraints:
	sample = "[a-zA-Z\d]+",
	clust = "[\d]+"


# rule all:
	# input:
		#expand("../../{sample}/{assembly}/asm.r_utg.fa", sample=sample),
		# expand("../../{sample}/{assembly}/SaaRclust/Clusters/hard_clusters.RData", sample=sample, assembly=assembly),
		# expand("/{sample}/split/{sample}_{clust}.gfa", sample=sample, clust=chromosomes)

tmp_ext = ['assembled.fastq', 'discarded.fastq', 'unassembled.forward.fastq', 'unassembled.reverse.fastq']

# rule temp_helper:
# 	output:
# 		# SaaRclust
# 		hard_clust="{sample}/SaaRclust/Clusters/hard_clusters.RData",
# 		soft_clust="{sample}/SaaRclust/Clusters/soft_clusters.RData",
# 		ML_clust="{sample}/SaaRclust/Clusters/MLclust.data",
# 		ss_clust="{sample}/SaaRclust/Clusters/ss_clusters.data",
# 		ss_clust_sp=expand("{{sample}}/SaaRclust/Clusters/ss_clusters_{clust}.data", clust=chromosomes),
# 		clust_pairs="{sample}/SaaRclust/Clusters/clust_partners.txt",
# 		wc_cells_clusters="{sample}/SaaRclust/Clusters/wc_cells_clusters.data"
rule temp:
	input:
		ref_aln = expand("{sample}/ref_aln/{sample}.bam", sample=sample),
		split_assembly_fasta_by_clust=expand("{sample}/clustered_ss_fasta/", sample=sample),
		split_ss_reads_by_clust=expand("{sample}/clustered_assembly_fasta/", sample=sample),
		detect_bubbles=  expand("{sample}/graph_components/simplified_assembly.gfa.json", sample=sample),
		fastmap_ss_reads_to_unitigs=expand("{sample}/exact_match/", sample=sample),
		output_valid_maps=expand("{sample}/valid_exact_match/", sample=sample),
		phase_unitigs=expand("{sample}/phasing/{sample}_phased_unitigs.data", sample=sample),
		summary_stats=expand("{sample}/phasing/{sample}_phased_summary_stats.csv", sample=sample),
		unmerged_SaaRclust_by_component = expand("{sample}/SaaRclust/Clusters/MLclust.data", sample=sample),
		# phase_unitigs=expand("{sample}/phasing/phased_unitigs_{component}.data", sample=sample, component=non_acrocentric_components),

###############################################################################
##############		Unmerged SS processing		###############
###############################################################################
rule unzip_ss:
	input: ss_dir+"/{lib}_{pair}"+ss_suffix,
	output: temp("{sample}/ss/unmerged/{lib}_{pair}.fasta")
	conda:'envs/haploclust_cl.yaml'
	log: "log/unzip_ss_{sample}_{lib}_{pair}.log"
	shell: "(time bioawk -c fastx '{{print \">\"$name; print $seq}}' <(cat {input}) > {output}) > {log} 2>&1"

rule add_ss_libname_unmerged:
	input: "{sample}/ss/unmerged/{lib}_{pair}.fasta"
	output: temp("{sample}/ss/unmerged/{lib}_{pair}.renamed.fasta")
	conda:'envs/haploclust_cl.yaml'
	log: "log/add_ss_libname_{sample}_{lib}_{pair}.log"
	shell:
		'''
		bioawk -c fastx -v libname={wildcards.lib} '{{print \">\"$name"_"libname; print $seq}}' <(cat {input}) > {output}
		'''

rule homopolymer_compress_unmerged_ss:
	input: "{sample}/ss/unmerged/{lib}_{pair}.renamed.fasta",
	output: "{sample}/ss/unmerged/{lib}_{pair}.homopolymer-compressed.fasta",
	conda:'envs/haploclust_pyenv.yaml'
	log: "log/compress_unmerged_ss_{sample}_{lib}_{pair}.log"
	shell:
		'''
		(python3 scripts/python/homopolymer_compress_fasta.py \\
		--input {input} \\
		--output {output}) > {log} 2>&1
		'''

###############################################################################
##############		merging read pairs with PEAR		###############
###############################################################################
# TODO check the name of the merged reads, does it simply take the name of the first read in the pair?
rule pear_merge_mates:
	input:
		fq1=ss_dir+"/{lib}_1"+ss_suffix,
		fq2=ss_dir+"/{lib}_2"+ss_suffix,
	output:
		"{sample}/ss/merged/{lib}.assembled.fastq",
		"{sample}/ss/merged/{lib}.discarded.fastq",
		"{sample}/ss/merged/{lib}.unassembled.forward.fastq",
		"{sample}/ss/merged/{lib}.unassembled.reverse.fastq"
	conda:'envs/haploclust_cl.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 4096 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/pear_merge_mates_{sample}_{lib}.log"
	benchmark: "benchmark/pear_merge_mates_{sample}_{lib}.benchmark"
	shell: "(pear -f {input.fq1} -r {input.fq2} -t 101 -o {wildcards.sample}/ss/merged/{wildcards.lib}) > {log} 2>&1"

rule concat_assembled_with_first_pair_of_unassembled:
	input:
		"{sample}/ss/merged/{lib}.assembled.fastq",
		"{sample}/ss/merged/{lib}.unassembled.forward.fastq",
	output: temp("{sample}/ss/merged/{lib}.combined.fasta")
	conda:'envs/haploclust_cl.yaml'
	log: "log/concat_merged_with_first_unmerged_{sample}_{lib}.log"
	shell: "(time bioawk -c fastx '{{print \">\"$name; print $seq}}' <(cat {input}) > {output}) > {log} 2>&1"


rule add_ss_libname:
	input: "{sample}/ss/merged/{lib}.combined.fasta"
	output: temp("{sample}/ss/merged/{lib}.combined.renamed.fasta")
	conda:'envs/haploclust_cl.yaml'
	log: "log/add_ss_libname_{sample}_{lib}.log"
	shell:
		'''
		bioawk -c fastx -v libname={wildcards.lib} '{{print \">\"$name"_"libname; print $seq}}' <(cat {input}) > {output}
		'''

rule homopolymer_compress_ss:
	input: "{sample}/ss/merged/{lib}.combined.renamed.fasta" # concat_assembled_with_first_pair_of_unassembled
	output: "{sample}/ss/merged/{lib}.combined.homopolymer-compressed.fasta"
	conda:'envs/haploclust_pyenv.yaml'
	log: "log/compress_ss_{sample}_{lib}.log"
	shell:
		'''
		python3 scripts/python/homopolymer_compress_fasta.py \\
		--input {input} \\
		--output {output}
		'''

# ########################################################################################################################
# ################################### hifiasm ##############################################
# ########################################################################################################################


rule gfa_to_fasta:
	input: assembly,
	output: "{sample}/{sample}_assembly.fa",
	threads: 2
	log: "log/gfa_to_fasta_{sample}.log"
	shell:
		'''
		(time grep S {input} | awk '{{print \">\"$2\"\\n\"$3}}' > {output}) > {log} 2<&1
		'''

# ##########################################################################################################################
# ######### TO REMOVE- Using reference genome only for evaluation- mapping/haplotagging overlap graph unitigs ##############
# ##########################################################################################################################
#
# db_prefix = 'reference/'+reference_name+'/'+reference_name
#
# rule bwa_index_ref:
# 	input: reference
# 	output: expand(db_prefix+".{suffix}", suffix=bwa_index_suffices)
# 	params:
# 		prefix=db_prefix
# 	conda:'envs/haploclust_cl.yaml'
# 	resources:
# 		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
# 		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
# 	log: "log/bwa_index_ref.log"
# 	benchmark: "benchmark/bwa_index_ref.benchmark"
# 	params: prefix=db_prefix
# 	shell: "(time bwa index -p {params.prefix} {input}) > {log} 2>&1"
#
# rule bwa_align_ss_to_ref:
# 	input:
# 		ref=reference,
# 		ref_index=expand(db_prefix+".{suffix}", suffix=bwa_index_suffices), # bwa_index_unitigs
# 		mate1=ss_dir+"/{lib}_1"+ss_suffix,
# 		mate2=ss_dir+"/{lib}_2"+ss_suffix
# 	output: "{sample}/ref_aln/ss/{sample}_{lib}.bam"
# 	params:
# 		prefix=db_prefix
# 	threads: 6
# 	conda:'envs/haploclust_cl.yaml'
# 	resources:
# 		mem_mb = lambda wildcards, attempt: 1024 * 16 * attempt,
# 		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
# 	log:
# 		bwa="log/bwa_align_ss_to_ref_bwa_{sample}_{lib}.log",
# 		samtools="log/bwa_align_ss_to_ref_samtools_{sample}_{lib}.log",
# 	benchmark: "benchmark/bwa_align_ss_to_ref_{sample}_{lib}.benchmark"
# 	shell:
# 		'''
# 		bwa mem -t {threads} -R "@RG\\tID:{wildcards.lib}" -v 2 {params.prefix} {input.mate1} {input.mate2} 2> {log.bwa} | samtools view -b -F 2304 /dev/stdin > {output} 2> {log.samtools}
# 		'''


# ##########################################################################################################################
# ######### TO REMOVE- Using reference genome only for evaluation- mapping/haplotagging overlap graph unitigs ##############
# ##########################################################################################################################

ref_out = 'reference/'+reference_name+'/'+reference_stem+'.homopolymer-compressed.fasta'
# print(ref_out)

rule homopolymer_compress_ref:
	input: reference
	output: ref_out
	conda:'envs/haploclust_pyenv.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 16 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/homopolymer_compress_ref.log"
	benchmark: "benchmark/homopolymer_compress_ref.benchmark"
	shell:
		'''
		python3 scripts/python/homopolymer_compress_fasta.py \\
		--input {input} \\
		--output {output}
		'''

rule bwa_index_homopolymer_compressed_ref:
	input: ref_out
	output: expand(ref_out+".{suffix}", suffix=bwa_index_suffices)
	conda:'envs/haploclust_cl.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/bwa_index_ref.log"
	benchmark: "benchmark/bwa_index_ref.benchmark"
	params: prefix=ref_out
	shell: "(time bwa index -p {params.prefix} {input}) > {log} 2>&1"

# ##########################################################################################################################
# ######### TO REMOVE- Using reference genome only for evaluation- mapping/haplotagging overlap graph unitigs ##############
# ##########################################################################################################################
#
# rule bwa_align_merged_ss_to_ref:
# 	input:
# 		ref=reference,
# 		ref_index=expand(db_prefix+".{suffix}", suffix=bwa_index_suffices), # bwa_index_unitigs
# 		ss="{sample}/ss/merged/{lib}.combined.fasta"
# 	output: "{sample}/ref_aln/merged_ss/{sample}_{lib}.bam"
# 	params:
# 		prefix=db_prefix
# 	threads: 6
# 	conda:'envs/haploclust_cl.yaml'
# 	resources:
# 		mem_mb = lambda wildcards, attempt: 1024 * 16 * attempt,
# 		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
# 	log:
# 		bwa="log/bwa_align_merged_ss_to_ref_bwa_{sample}_{lib}.log",
# 		samtools="log/bwa_align_merged_ss_to_ref_samtools_{sample}_{lib}.log",
# 	benchmark: "benchmark/bwa_align_ss_to_ref_{sample}_{lib}.benchmark"
# 	shell:
# 		'''
# 		bwa mem -t {threads} -R "@RG\\tID:{wildcards.lib}" -v 2 {params.prefix} {input.ss} 2> {log.bwa} | samtools view -b -F 2304 /dev/stdin > {output} 2> {log.samtools}
# 		'''

# ##########################################################################################################################
# ######### TO REMOVE- Using reference genome only for evaluation- mapping/haplotagging overlap graph unitigs ##############
# ##########################################################################################################################

rule bwa_align_unmerged_compressed_ss_to_ref:
	input:
		ref=ref_out,
		ref_index=expand(ref_out+".{suffix}", suffix=bwa_index_suffices),
		mate1="{sample}/ss/unmerged/{lib}_1.homopolymer-compressed.fasta",
		mate2="{sample}/ss/unmerged/{lib}_2.homopolymer-compressed.fasta",
	output: "{sample}/ref_aln/unmerged_compressed_ss/{sample}_{lib}.bam"
	threads: 6
	conda:'envs/haploclust_cl.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 16 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log:
		bwa="log/bwa_align_unmerged_ss_to_ref_bwa_{sample}_{lib}.log",
		samtools="log/bwa_align_unmerged_ss_to_ref_samtools_{sample}_{lib}.log",
	shell:
		'''
		bwa mem -t {threads} -R "@RG\\tID:{wildcards.lib}" -v 2 {input.ref} {input.mate1} {input.mate2} 2> {log.bwa} | samtools view -b -F 2304 /dev/stdin > {output} 2> {log.samtools}
		'''
#
# rule bwa_align_merged_compressed_ss_to_ref:
# 	input:
# 		ref=ref_out,
# 		ref_index=expand(ref_out+".{suffix}", suffix=bwa_index_suffices), # bwa_index_unitigs
# 		ss="{sample}/ss/merged/{lib}.combined.homopolymer-compressed.fasta"
# 	output: "{sample}/ref_aln/merged_compressed_ss/{sample}_{lib}.bam"
# 	threads: 6
# 	conda:'envs/haploclust_cl.yaml'
# 	resources:
# 		mem_mb = lambda wildcards, attempt: 1024 * 16 * attempt,
# 		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
# 	log:
# 		bwa="log/bwa_align_merged_compressed_ss_to_ref_bwa_{sample}_{lib}.log",
# 		samtools="log/bwa_align_merged_compressed_ss_to_ref_samtools_{sample}_{lib}.log",
# 	benchmark: "benchmark/bwa_align_merged_compressed_ss_to_ref_{sample}_{lib}.benchmark"
# 	shell:
# 		'''
# 		bwa mem -t {threads} -R "@RG\\tID:{wildcards.lib}" -v 2 {input.ref} {input.ss} 2> {log.bwa} | samtools view -b -F 2304 /dev/stdin > {output} 2> {log.samtools}
# 		'''

# ##########################################################################################################################
# ######### TO REMOVE- Using reference genome only for evaluation- mapping/haplotagging overlap graph unitigs ##############
# ##########################################################################################################################

rule map_unitigs_to_ref:
	input:
		ref=ref_out,
		bwa_idx=expand(ref_out+".{suffix}", suffix=bwa_index_suffices), # bwa_index_ref
		unitigs="{sample}/{sample}_assembly.fa" # gfa_to_fasta
	output: "{sample}/ref_aln/unsorted_{sample}.bam"
	conda:'envs/haploclust_cl.yaml'
	log: "log/map_unitigs_to_ref_{sample}.log"
	benchmark: "benchmark/map_unitigs_to_ref_{sample}.benchmark"
	threads: 24 # needs a single server w/ 24 cores ~ less portable. Move to config at some point
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 32 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	shell:
		"(time minimap2 -t {threads} --secondary=no --eqx -Y -ax asm20 {input.ref} {input.unitigs} | samtools view -Sb - > {output}) > {log} 2>&1"

rule sort_index_unitig_bams:
	input: "{sample}/ref_aln/unsorted_{sample}.bam" # map_unitigs_to_ref
	output:
		bam="{sample}/ref_aln/{sample}.bam",
		bai="{sample}/ref_aln/{sample}.bam.bai"
	conda:'envs/haploclust_cl.yaml'
	threads: 8
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 16 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/bwa_sort_index_unitigs_bams_{sample}.log"
	benchmark: "benchmark/bwa_sort_index_unitigs_bams_{sample}.benchmark"
	shell:
		'''
		(time samtools sort -@ {threads} -o {output.bam} {input} &&
		samtools index -@ {threads} {output.bam}) > {log} 2>&1
		'''

########################################################################################################################
###################################  aligning ss reads to hifiasm unitigs ##############################################
########################################################################################################################

rule bwa_index_unitigs:
	input: "{sample}/{sample}_assembly.fa" # gfa_to_fasta
	output: expand("{{sample}}/{{sample}}_assembly.fa.{bwa_index_suffix}", bwa_index_suffix=bwa_index_suffices)
	conda:'envs/haploclust_cl.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
		walltime = lambda wildcards, attempt: f'{8 + attempt*attempt:02}:59:00'
	log: "log/bwa_index_hifiasm_unitigs_{sample}.log"
	benchmark: "benchmark/bwa_index_unitigs_{sample}.benchmark"
	shell: "(time bwa index {input}) > {log} 2>&1"

rule bwa_align_unmerged_compressed_ss_to_unitigs:
	input:
		unitigs="{sample}/{sample}_assembly.fa", # gfa_to_fasta
		unitigs_index=expand("{{sample}}/{{sample}}_assembly.fa.{bwa_index_suffix}", bwa_index_suffix=bwa_index_suffices), # bwa_index_unitigs
		mate1="{sample}/ss/unmerged/{lib}_1.homopolymer-compressed.fasta",
		mate2="{sample}/ss/unmerged/{lib}_2.homopolymer-compressed.fasta",
	output: temp("{sample}/temp_unmerged_bwa_ss_unitigs/unmerged_compressed_ss/{lib}.bam")
	threads: 6
	conda:'envs/haploclust_cl.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 16 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log:
		bwa="log/bwa_align_unmerged_compressed_ss_to_unitigs_bwa_{sample}_{lib}.log",
		samtools="log/bwa_align_unmerged_compressed_ss_to_unitigs_samtools_{sample}_{lib}.log",
	shell:
		'''
		bwa mem -t {threads} -R "@RG\\tID:{wildcards.lib}" -v 2 {input.unitigs} {input.mate1} {input.mate2} 2> {log.bwa} | samtools view -b -F 2304 /dev/stdin > {output} 2> {log.samtools}
 		'''

rule bwa_sort_unmerged_compressed_ss_to_unitigs:
	input:  "{sample}/temp_unmerged_bwa_ss_unitigs/unmerged_compressed_ss/{lib}.bam" # bwa_align
	output: "{sample}/unmerged_bwa_ss_unitigs/unmerged_compressed_ss/{lib}.bam"
	conda:'envs/haploclust_cl.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/bwa_sort_unmerged_{sample}_{lib}.log"
	benchmark: "benchmark/bwa_sort_unmerged_{sample}_{lib}.benchmark"
	shell:
		'''
		(time samtools sort -o {output} {input}) > {log} 2>&1
		'''

rule unmerged_mark_duplicates:
	input:  "{sample}/unmerged_bwa_ss_unitigs/unmerged_compressed_ss/{lib}.bam" # bwa_sort
	output: "{sample}/unmerged_bwa_ss_unitigs/unmerged_compressed_ss/{lib}.mdup.bam",
	conda:'envs/haploclust_cl.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/unmerged_mark_duplicates_{sample}_{lib}.log"
	benchmark: "benchmark/unmerged_mark_duplicates_{sample}_{lib}.benchmark"
	shell:
		'''
		(time sambamba markdup {input} {output}) > {log} 2>&1
		'''

rule unmerged_bwa_index:
	input:  "{sample}/unmerged_bwa_ss_unitigs/unmerged_compressed_ss/{lib}.mdup.bam" # mark_duplicates
	output: "{sample}/unmerged_bwa_ss_unitigs/unmerged_compressed_ss/{lib}.mdup.bam.bai"
	conda:'envs/haploclust_cl.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/unmerged_bwa_index_{sample}_{lib}.log"
	benchmark: "benchmark/unmerged_bwa_index_{sample}_{lib}.benchmark"
	shell:
		'''
		(time samtools index {input}) > {log} 2>&1
		'''
#
# rule bwa_align:
# 	input:
# 		unitigs=expand("{{sample}}/{{sample}}_{assembly_stem}.fa", assembly_stem=assembly_stem), # gfa_to_fasta
# 		unitigs_index=expand("{{sample}}/{{sample}}_{assembly_stem}.fa.{extention}", assembly_stem=assembly_stem, extention=bwa_index_suffices), # bwa_index_unitigs
# 		ss_reads="{sample}/ss/merged/{lib}.combined.homopolymer-compressed.fasta" # homopolymer_compress_ss
# 	output: temp("{sample}/temp_bwa_ss_unitigs/{lib}.bam")
# 	threads: 2
# 	conda:'envs/haploclust_cl.yaml'
# 	resources:
# 		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
# 		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
# 	log: "log/bwa_align_chunks_{sample}_{lib}.log"
# 	benchmark: "benchmark/bwa_align_chunks_{sample}_{lib}.benchmark"
# 	shell: "(time bwa mem -t {threads} {input.unitigs} {input.ss_reads} | samtools view -Sb - > {output}) > {log} 2>&1"
#
# rule bwa_sort:
# 	input:  "{sample}/temp_bwa_ss_unitigs/{lib}.bam" # bwa_align
# 	output: "{sample}/bwa_ss_unitigs/{lib}.bam"
# 	conda:'envs/haploclust_cl.yaml'
# 	resources:
# 		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
# 		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
# 	log: "log/bwa_sort_{sample}_{lib}.log"
# 	benchmark: "benchmark/bwa_sort_{sample}_{lib}.benchmark"
# 	shell:
# 		'''
# 		(time samtools sort -o {output} {input}) > {log} 2>&1
# 		'''
#
# rule mark_duplicates:
# 	input:  "{sample}/bwa_ss_unitigs/{lib}.bam" # bwa_sort
# 	output: "{sample}/bwa_ss_unitigs/{lib}.mdup.bam",
# 	conda:'envs/haploclust_cl.yaml'
# 	resources:
# 		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
# 		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
# 	log: "log/mark_duplicates_{sample}_{lib}.log"
# 	benchmark: "benchmark/mark_duplicates_{sample}_{lib}.benchmark"
# 	shell:
# 		'''
# 		(time sambamba markdup {input} {output}) > {log} 2>&1
# 		'''
#
# rule bwa_index:
# 	input:  "{sample}/bwa_ss_unitigs/{lib}.mdup.bam" # mark_duplicates
# 	output: "{sample}/bwa_ss_unitigs/{lib}.mdup.bam.bai"
# 	conda:'envs/haploclust_cl.yaml'
# 	resources:
# 		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
# 		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
# 	log: "log/bwa_index_{sample}_{lib}.log"
# 	benchmark: "benchmark/bwa_index_{sample}_{lib}.benchmark"
# 	shell:
# 		'''
# 		(time samtools index {input}) > {log} 2>&1
# 		'''
# ###########################################################################################################################
# ################################### splitting reads/unitigs into components ##############################################
# ###########################################################################################################################

rule assembly_components_gfa:
	input: assembly,
	output: expand("{{sample}}/graph_components/component{component}.gfa",component=components)
	params:
		n=num_components,
		out_dir = "{sample}/graph_components/",
	conda:'envs/haploclust_gfasubgraphs.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/assembly_components_gfa_{sample}.log"
	shell:
		'''
		(time GFASubgraph -g {input} output_comps --seq-size -n {params.n} --output_dir {params.out_dir} ) > {log} 2<&1
		'''

# ########################################################################################################################
# ###################################  SaaRclust ##############################################
# ########################################################################################################################

# NOTE currently the gfa component are only used to extract the long unitigs, though hopefully more specific component information will be used in the future.
rule unmerged_contibait_SaaRclust:
	input:
		bam=expand("{{sample}}/unmerged_bwa_ss_unitigs/unmerged_compressed_ss/{lib}.mdup.bam", lib=libs), # mark_duplicates
		bai=expand("{{sample}}/unmerged_bwa_ss_unitigs/unmerged_compressed_ss/{lib}.mdup.bam.bai", lib=libs), # bwa_index
		gfa=expand("{{sample}}/graph_components/component{component}.gfa", component=components)
	output:
		ML_clust="{sample}/SaaRclust/Clusters/MLclust.data",
		ss_clust="{sample}/SaaRclust/Clusters/ss_clusters.data",
		# ss_clust_sp=directory("{sample}/SaaRclust/Clusters/ss_clusters/"),
		clust_pairs="{sample}/SaaRclust/Clusters/clust_partners.txt",
		wc_cells_clusters="{sample}/SaaRclust/Clusters/wc_cells_clusters.data",
		unclustered='{sample}/SaaRclust/Clusters/SaaRclust_unclustered_rnames.tsv'
	params:
		prefix = expand("{sample}/SaaRclust/", sample=sample),
		EMiter=EMiter,
		segment_length_threshold=segment_length_threshold
	singularity: "haploclust_Renv2.sif"
	# conda: "envs/haploclust_Renv.yaml"
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 32 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	threads: 8 # 24
	log: "log/SaaRclust_by_component_{sample}_initial_clusters.log"
	benchmark: "benchmark/SaaRclust_by_component_{sample}_initial_clusters.benchmark"
	shell:
	 	'''
		Rscript --vanilla scripts/R/unmerged_SaaRclust_contibait.snakemake.R \\
		--bam {input.bam} \\
		--gfa {input.gfa} \\
		--output-prefix {params.prefix} \\
		--em-iter {params.EMiter} \\
		--threads {threads} \\
		--segment-length-threshold {params.segment_length_threshold} \\
		--log {log}
		'''



# ###########################################################################################################################
# ################################### splitting reads/unitigs into components ##############################################
# ###########################################################################################################################


rule merge_all_ss_fasta:
	input: expand("{{sample}}/ss/merged/{lib}.combined.homopolymer-compressed.fasta", lib=libs) # homopolymer_compress_ss
	output: temp("{sample}/ss/split/all_libs.fasta")
	conda:'envs/haploclust_cl.yaml'
	log: "log/merge_all_ss_fasta_{sample}.log"
	benchmark: "benchmark/merge_all_ss_fasta_{sample}.benchmark"
	shell:
		'''
		for f in {input}
		do
			libname=$(basename $f .combined.homopolymer-compressed.fasta)
			echo libname = $libname
			bioawk -c fastx '{{print \">\"$name; print $seq}}' $f >> {output}
		done
		'''

# TODO sanitize the outputs for these rules ~ they don't make folders if they don't exist
rule split_ss_reads_by_clust:
	input:
		fasta="{sample}/ss/split/all_libs.fasta", # merge_all_ss_fasta
		component="{sample}/SaaRclust/Clusters/ss_clusters.data", # SaaRclust
	output: directory("{sample}/clustered_ss_fasta/")
	conda:'envs/haploclust_pyenv.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 16 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/split_ss_reads_{sample}.log"
	benchmark: "benchmark/split_ss_reads_{sample}.benchmark"
	shell: # mkdir appears to be neeeded when directory is specified for output
		'''
		(time mkdir {output} &&
		python3 scripts/python/whatshap_split.py {input.fasta} {input.component}  --output-folder {output} --split-by-cluster) > {log} 2>&1
		'''
rule split_assembly_fasta_by_clust:
	input:
		fasta="{sample}/{sample}_assembly.fa", #gfa to fasta
		clust="{sample}/SaaRclust/Clusters/MLclust.data", # SaarClust
	output:
		directory("{sample}/clustered_assembly_fasta/"),
	conda:'envs/haploclust_pyenv.yaml'
	log: "log/split_assembly_graph_{sample}.log"
	benchmark: "benchmark/split_assembly_graph_{sample}.benchmark"
	shell: # mkdir appears to be neeeded when directory is specified for output
		'''
		(time mkdir {output} &&
		python3 scripts/python/whatshap_split.py {input.fasta} {input.clust} --output-folder {output} --split-by-cluster) > {log} 2>&1
		'''


########################################################################################################################
##################################### Detecting Bubbles in Hifiasm Graph ###############################################
########################################################################################################################
rule simplify_assembly:
	input:
		assembly=assembly,
		clusters="{sample}/SaaRclust/Clusters/MLclust.data"
	output: "{sample}/graph_components/simplified_assembly.gfa",
	params:
		segment_length_threshold = 99999999999 # only want to simplfy the unitigs in each cluster, with no additional unitigs included ~ give high threshold that won't be surpassed.
	conda:'envs/haploclust_gfa.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 16 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/simplify_assembly_{sample}.log"
	shell:
		'''
		(python3 scripts/python/simplify_gfa.py \\
		--input {input.assembly} \\
		--output {output} \\
		--segment-length-threshold {params.segment_length_threshold} \\
		--clusters {input.clusters}) > {log} 2>&1
		'''

rule detect_bubbles:
	input: "{sample}/graph_components/simplified_assembly.gfa"
	output: "{sample}/graph_components/simplified_assembly.gfa.json"
	conda:'envs/haploclust_bubblegun.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/detect_bubbles_assembly_{sample}.log"
	benchmark: "benchmark/detect_bubbles_assembly_{sample}.benchmark"
	shell: "(time BubbleGun -g {input} bchains --only_simple --bubble_json {output})> {log} 2>&1"

########################################################################################################################
##################################### Mapping Strand-seq read to Bubbles ###############################################
########################################################################################################################

# rule make_clust_fasta_file_lists:
# 	input: "{sample}/{clust_folder}/"
# 	output: "{sample}/{clust_folder}/file_list.txt"
# 	shell: "find {input} -type f | grep '.fasta$'| sort > {output}"


rule fastmap_ss_reads_to_unitigs:
	input:
		SSreads="{sample}/clustered_ss_fasta/", # make_clust_fasta_file_lists
		utg="{sample}/clustered_assembly_fasta/" # make_clust_fasta_file_lists
	output:
		map=directory("{sample}/exact_match/") #component{component}_maximal_unique_exact_match.data",
	params:
		utg_prefix= expand("{sample}_assembly_", sample=sample),
		ss_prefix = "all_libs_"
	conda:'envs/haploclust_cl.yaml'
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	threads: 2
	log: "log/map_SS_reads_to_unitigs_{sample}.log"
	benchmark: "benchmark/map_SS_reads_to_unitigs_{sample}.benchmark"
	shell: # TODO why utigs .fa and SS .fasta? TODO maybe a clever way to parallelize this even when number of files is unknown ahead of time?
		'''
		mkdir {output} &&(
			NFILES=$(ls {input.SSreads} | grep ".fasta$" | wc -l)
			echo $NFILES
			for ((i=1; i<=NFILES; i++))
			do
				utig={input.utg}/{params.utg_prefix}$i.fa
				ss={input.SSreads}/{params.ss_prefix}$i.fasta
				out={output}/maximal_unique_exact_match_$i.data
				echo $i
				echo $utig
				echo $ss
				echo $out
				(time bwa index $utig && bwa fastmap -w 1 -l 75 $utig $ss > $out) >> {log} 2>&1
			done
		)
		'''

		# '''
		# paste -d '\n' {input.utg} {input.SSreads}  | xargs -n2 sh -c '(time bwa index $0 && bwa fastmap -w 1 -l 75 $0 $1 > {output}) > {log} 2>&1'
		# '''


# rule map_ss_reads_to_unitigs:
# 	input:
# 		SSreads=lambda wildcards: "{sample}/ss/split/all_libs_" + str(int(wildcards.component)-1) + ".fasta", # split_ss_reads
# 		utg=expand("{{sample}}/{assembly_stem}_graph_components/component{{component}}.fa", assembly_stem=assembly_stem), # component_gfa_to_fasta
# 	output:
# 		map="{sample}/exact_match/component{component}_maximal_unique_exact_match.data",
# 	conda:'envs/haploclust_cl.yaml'
# 	resources:
# 		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
# 		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
# 	threads: 2
# 	log: "log/map_SS_reads_to_unitigs_{sample}_{component}.log"
# 	benchmark: "benchmark/map_SS_reads_to_unitigs_{sample}_{component}.benchmark"
# 	shell:
# 		'''
# 		(time bwa index {input.utg} &&
# 		bwa fastmap -w 1 -l 75 {input.utg} {input.SSreads} > {output}) > {log} 2>&1
# 		'''

rule output_valid_maps:
	input:
		map="{sample}/exact_match/", # map_ss_reads_to_unitigs
		bubbles="{sample}/graph_components/simplified_assembly.gfa.json", # detect_bubbles_overlap_graph
	output: directory("{sample}/valid_exact_match/") # valid_{component}_maximal_unique_exact_match.data"
	params:
		map_prefix= "maximal_unique_exact_match_",
	conda:'envs/haploclust_pyenv.yaml'
	# resources:
	# 	mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
	# 	walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/output_valid_maps_{sample}.log"
	benchmark: "benchmark/output_valid_maps_{sample}.benchmark"
	shell:
		'''
		mkdir {output} &&(
			NFILES=$(ls {input.map} | wc -l)
			echo $NFILES
			for ((i=1; i<=NFILES; i++))
			do
				map={input.map}/{params.map_prefix}$i.data
				out={output}/valid_maximal_unique_exact_match_$i.data
				echo $i
				echo $map
				echo $out
				python3 scripts/python/output_valid_maps.snakemake.py \\
				--bubbles {input.bubbles} \\
				--map $map \\
				--output $out
			done
		) > {log} 2>&1
		'''
	# '''
	# python3 scripts/python/output_valid_maps.snakemake.py \\
	# --bubbles {input.bubbles} \\
	# --map {input.map} \\
	# --output {output} && touch {output}
	# '''

########################################################################################################################
#################################### Phasing Strand-seq reads, bubbles, and Unitigs ####################################
########################################################################################################################

rule phase_unitigs:
	input:
		ss_clust="{sample}/SaaRclust/Clusters/ss_clusters.data", # SaaRclust
		unitigs_clust="{sample}/SaaRclust/Clusters/MLclust.data", # SaaRclust
		clust_pairs="{sample}/SaaRclust/Clusters/clust_partners.txt", # SaaRclust
		map="{sample}/valid_exact_match/", # output_valid_maps
		bubbles="{sample}/graph_components/simplified_assembly.gfa.json", # detect_bubbles_overlap_graph
		wc_cell_clust="{sample}/SaaRclust/Clusters/wc_cells_clusters.data", # SaaRclust
	output:"{sample}/phasing/{sample}_phased_unitigs.data",
	singularity: "haploclust_Renv2.sif"
	# conda: "envs/haploclust_Renv.yaml"
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 32 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/phase_unitigs_{sample}.log"
	benchmark: "benchmark/phase_unitigs_{sample}.benchmark"
	shell:
		'''
   		Rscript --vanilla scripts/R/phase_bubbles.snakemake.R \\
   		--clust-pairs {input.clust_pairs} \\
   		--wc-cell-clust {input.wc_cell_clust} \\
		--ss-clust {input.ss_clust} \\
		--unitig-clust {input.unitigs_clust} \\
		--map {input.map} \\
		--bubbles {input.bubbles} \\
		--sample {wildcards.sample} \\
   		--output {output} \\
   		--log {log}
   		'''

rule phase_summary_stats:
	input:
		phase_files="{sample}/phasing/{sample}_phased_unitigs.data",
		gfa=expand("{{sample}}/graph_components/component{component}.gfa", component=components)
	output:"{sample}/phasing/{sample}_phased_summary_stats.csv",
	singularity: "haploclust_Renv2.sif"
	resources:
		mem_mb = lambda wildcards, attempt: 1024 * 4 * attempt,
		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
	log: "log/phase_summary_{sample}.log"
	shell:
		'''
   		Rscript --vanilla scripts/R/summarise_phasing.snakemake.R \\
		--phase-files {input.phase_files} \\
		--gfa {input.gfa} \\
   		--output {output} \\
   		--log {log}
   		'''

#
# rule phase_unitigs:
# 	input:
# 		ss_clust="{sample}/SaaRclust/Clusters/ss_clusters_component{component}.data", # SaaRclust
# 		clust_pairs="{sample}/SaaRclust/Clusters/clust_partners.txt", # SaaRclust
# 		map="{sample}/exact_match/valid_{component}_maximal_unique_exact_match.data", # output_valid_maps
# 		bubbles=expand("{{sample}}/{assembly_stem}_graph_components/simplified_component{{component}}.gfa.json",assembly_stem=assembly_stem), # detect_bubbles_overlap_graph
# 		wc_cell_clust="{sample}/SaaRclust/Clusters/wc_cells_clusters.data", # SaaRclust
# 	output:"{sample}/phasing/phased_unitigs_{component}.data",
# 	params:
# 		clust=lambda wildcards: str(int(wildcards.component)-1)
# 	singularity: "haploclust_Renv2.sif"
# 	# conda: "envs/haploclust_Renv.yaml"
# 	resources:
# 		mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
# 		walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
# 	log: "log/phase_unitigs_{sample}_{component}.log"
# 	benchmark: "benchmark/phase_unitigs_{sample}_{component}.benchmark"
# 	shell:
# 		'''
#    		Rscript --vanilla scripts/R/phase_bubbles.snakemake.R \\
#    		--clust-pairs {input.clust_pairs} \\
#    		--wc-cell-clust {input.wc_cell_clust} \\
# 		--ss-clust {input.ss_clust} \\
# 		--map {input.map} \\
# 		--bubbles {input.bubbles} \\
# 		--sample {wildcards.sample} \\
# 		--clust {params.clust} \\
#    		--output {output} \\
#    		--log {log}
#    		'''

# 		'''
